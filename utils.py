import torch
from torch import is_tensor

from transformers.activations import ACT2FN

from transformer_lens import HookedTransformer

from datasets import Dataset

VALUES_TO_SUMMARISE = ['hook_post', 'hook_pre_linear', 'hook_pre', 'swish']
CASES = ['gate+_in+',
        'gate+_in-',
        'gate-_in+',
        'gate-_in-']
_relevant_signs = {
    case : {
        'hook_pre_linear': +1 if case[-1]=='+' else -1,
        'hook_pre': +1 if case.startswith('gate+') else -1,
    }
    for case in CASES
}
RELEVANT_SIGNS = {
    case : {
        **_relevant_signs[case],
        'swish': -1,
        'hook_post' : _relevant_signs[case]['hook_pre_linear']*_relevant_signs[case]['hook_pre'],
    }
    for case in CASES
}

def get_act_type_keys(key):
    if key[0].startswith('gate+') and key[1]=='swish':
        return []
    extra_keys = [f'{key[0]}_{key[1]}']
    if key[0].startswith('gate+') and key[1]=='hook_pre':
        extra_keys.append(f'{key[0]}_swish')
    keys = extra_keys+VALUES_TO_SUMMARISE
    return keys

def detect_cases(gate_values, in_values, keys=None):
    """Given two tensors (or arrays),
    return a dictionary with up to four 0-1 tensors (arrays)
    indicating the four possible sign combinations.
    Ignores exact zeros (which probably come from padding tokens).

    Args:
        gate_values (Tensor or any type that supports >, ~ and * operations): "gate" values
        in_values (same): "in" values
        keys (list[str]|None): the cases we are interested in. Defaults to None (all four cases).

    Returns:
        dict[str, Tensor]:
            keys are the ones in CASES,
            values are 0-1 tensors of the same shape as gate_values and in_values.
    """
    # gate_positive = gate_values>0
    # in_positive = in_values>0
    if keys is None:
        keys=CASES
    bins={}
    if 'gate+_in+' in keys:
        bins['gate+_in+'] = (gate_values>0)*(in_values>0)
    if 'gate+_in-' in keys:
        bins['gate+_in-'] = (gate_values>0)*(in_values<0)
    if 'gate-_in+' in keys:
        bins['gate-_in+'] = (gate_values<0)*(in_values>0)
    if 'gate-_in-' in keys:
        bins['gate-_in-'] = (gate_values<0)*(in_values<0)
    return bins

def refactor_glu(summary_dict, sign_to_adapt):
    """
    ONLY WORKS WITH SUMMARY FILES GENERATED BY AN OLD VERSION (BEFORE WE INTRODUCED PREPROCESSING)
    
    Adapt the summary_dict generated by b_activations.py to the refactored model.
    For the neurons that have changed under the weight preprocessing,
    the following changes are made:
    - max_activations and min_activations are exchanged, and their "values" entries multiplied by -1
    - summary_mean is multiplied by -1.

    Args:
        summary_dict (dict):
            The summary_dict generated by b_activations.py.
            Its entries are given by SUMMARY_KEYS and HEAP_KEYS in b_activations.py
            The values of the HEAP_KEYS are themselves dicts with keys "values", "indices".
        sign_to_adapt (tensor):
            tensor of +1 and -1, shape (layer, neuron). The -1 entries are the neurons to refactor.

    Returns:
        dict: the new version of summary_dict, compatible with the model.
        
    """
    #summary_mean is easy
    summary_dict["summary_mean"]*=sign_to_adapt
    #exchange max and min activations
    sign_to_adapt = sign_to_adapt.unsqueeze(0)
    max_activations_values = torch.where(
        sign_to_adapt==1,
        summary_dict["max_activations"]["values"],
        -summary_dict["min_activations"]["values"],
    )
    min_activations_values = torch.where(
        sign_to_adapt==1,
        summary_dict["min_activations"]["values"],
        -summary_dict["max_activations"]["values"],
    )
    summary_dict["max_activations"]["values"] = max_activations_values
    summary_dict["min_activations"]["values"] = min_activations_values
    max_activations_indices = torch.where(
        sign_to_adapt==1,
        summary_dict["max_activations"]["indices"],
        summary_dict["min_activations"]["indices"],
    )
    min_activations_indices = torch.where(
        sign_to_adapt==1,
        summary_dict["min_activations"]["indices"],
        summary_dict["max_activations"]["indices"],
    )
    summary_dict["max_activations"]["indices"] = max_activations_indices
    summary_dict["min_activations"]["indices"] = min_activations_indices
    return summary_dict

def adapt_activations(dict_all):
    #TODO adapt to new format
    """adapt activations incl. order of examples to cases in which refactoring switched the sign.

    Args:
        dict_all (dict): raw activations for a given neuron (without refactoring)

    Returns:
        dict: same but adapted
    """
    topk = dict_all['acts'].shape[0]//2
    new_dict = {
        key: torch.cat((-value[topk:], -value[:topk]))
        for key,value in dict_all.items()
    }
    return new_dict

class ModelWrapper(HookedTransformer):
    """Allows to directly access the (sub) activation function of the model,
    (i.e., Swish in the case of SwiGLU etc.)
    without looking it up every time.
    Initialise directly with model = ModelWrapper.from_pretrained(...)
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.actfn = ACT2FN[self.cfg.act_fn]

# class DatasetWrapper(Dataset):
#     """Allows to directly access the following properties of a dataset:
#         self.n_tokens: total number of tokens
#         self.max_seq_len: maximum length in tokens of a single example
#     You should FIRST create the Dataset object (e.g. with ds = datasets.load_from_disk(...))
#     and only THEN call ds = DatasetWrapper(ds).
#     """
#     _n_tokens=None
#     _max_seq_len=None
#     # def __init__(self, *args, **kwargs):
#     #     super().__init__(*args, **kwargs)
#     #     self.add_properties()

#     # @classmethod
#     # def load_from_disk(cls, *args, **kwargs):
#     #     #necessary because apparently load_from_disk does not call __init__ under the hood
#     #     base_dataset = super().load_from_disk(*args, **kwargs)
#     #     base_dataset.add_properties()

#     # @staticmethod
#     # def add_properties(dataset:Dataset):
#     #     self.n_tokens = sum(len(row) for row in self['input_ids'])
#     #     self.max_seq_len = max(len(row) for row in self['input_ids'])
#     def n_tokens(self):
#         if not self._n_tokens is not None:
#             self._n_tokens = sum(len(row) for row in self['input_ids'])
#         return self._n_tokens
#     def max_seq_len(self):
#         if not self._max_seq_len is not None:
#             self._max_seq_len = max(len(row) for row in self['input_ids'])
#         return self._max_seq_len

def add_properties(dataset:Dataset):
    setattr(dataset, "n_tokens", sum(len(row) for row in dataset['input_ids']))
    setattr(dataset, "max_seq_len", max(len(row) for row in dataset['input_ids']))
    return

def _move_to(dict_of_tensors, device):
    for key,value in dict_of_tensors.items():
        if is_tensor(value):
            dict_of_tensors[key] = value.to(device)
        elif isinstance(value, dict):
            dict_of_tensors[key] = _move_to(value, device)
    return dict_of_tensors

def topk_indices(maxact, k=16, largest=True, use_cuda=True):
    """Wrapper around torch.topk. Not actually used in the current version."""
    if use_cuda and torch.cuda.is_available():
        maxact = maxact.cuda()
    _values, indices = torch.topk(maxact, k=k, dim=0, largest=largest)
    #sample layer neuron -> k layer neuron, entries are indices along sample dimension
    return indices
