import torch
from torch import is_tensor

from datasets import Dataset
from transformers.activations import ACT2FN

from transformer_lens import HookedTransformer

def refactor_glu(summary_dict, sign_to_adapt):
    """Adapt the summary_dict generated by b_activations.py to the refactored model.
    For the neurons that have changed under the weight preprocessing,
    the following changes are made:
    - max_activations and min_activations are exchanged, and their "values" entries multiplied by -1
    - summary_mean is multiplied by -1.

    Args:
        summary_dict (dict):
            The summary_dict generated by b_activations.py.
            Its entries are given by SUMMARY_KEYS and HEAP_KEYS in b_activations.py
            The values of the HEAP_KEYS are themselves dicts with keys "values", "indices".
        sign_to_adapt (tensor):
            tensor of +1 and -1, shape (layer, neuron). The -1 entries are the neurons to refactor.

    Returns:
        dict: the new version of summary_dict, compatible with the model.
        
    """
    #summary_mean is easy
    summary_dict["summary_mean"]*=sign_to_adapt
    #exchange max and min activations
    sign_to_adapt = sign_to_adapt.unsqueeze(0)
    max_activations_values = torch.where(
        sign_to_adapt==1,
        summary_dict["max_activations"]["values"],
        -summary_dict["min_activations"]["values"],
    )
    min_activations_values = torch.where(
        sign_to_adapt==1,
        summary_dict["min_activations"]["values"],
        -summary_dict["max_activations"]["values"],
    )
    summary_dict["max_activations"]["values"] = max_activations_values
    summary_dict["min_activations"]["values"] = min_activations_values
    max_activations_indices = torch.where(
        sign_to_adapt==1,
        summary_dict["max_activations"]["indices"],
        summary_dict["min_activations"]["indices"],
    )
    min_activations_indices = torch.where(
        sign_to_adapt==1,
        summary_dict["min_activations"]["indices"],
        summary_dict["max_activations"]["indices"],
    )
    summary_dict["max_activations"]["indices"] = max_activations_indices
    summary_dict["min_activations"]["indices"] = min_activations_indices
    return summary_dict

def adapt_activations(dict_all):
    topk = dict_all['acts'].shape[0]//2
    new_dict = {
        key: torch.cat((-value[topk:], -value[:topk]))
        for key,value in dict_all.items()
    }
    return new_dict

class ModelWrapper(HookedTransformer):
    """Allows to directly access the (sub) activation function of the model,
    (i.e., Swish in the case of SwiGLU etc.)
    without looking it up every time
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.actfn = ACT2FN[self.cfg.act_fn]

class DatasetWrapper(Dataset):
    """Allows to directly access the following properties of a dataset:
        self.n_tokens: total number of tokens
        self.max_seq_len: maximum length in tokens of a single example
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.n_tokens = sum(len(row) for row in self['input_ids'])
        self.max_seq_len = max(len(row) for row in self['input_ids'])

def _move_to(dict_of_tensors, device):
    for key,value in dict_of_tensors.items():
        if is_tensor(value):
            dict_of_tensors[key] = value.to(device)
        elif isinstance(value, dict):
            dict_of_tensors[key] = _move_to(value, device)
    return dict_of_tensors

def topk_indices(maxact, k=16, largest=True, use_cuda=True):
    """Wrapper around torch.topk. Not actually used in the current version."""
    if use_cuda and torch.cuda.is_available():
        maxact = maxact.cuda()
    _values, indices = torch.topk(maxact, k=k, dim=0, largest=largest)
    #sample layer neuron -> k layer neuron, entries are indices along sample dimension
    return indices
